{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M132A HW3 (coding questions)\n",
    "## William Mahnke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8.1 $f(x_1,x_2) = x_1 + \\frac{1}{2}x_2 + \\frac{1}{2}x_1^2+x_2^2+3$\n",
    "\n",
    "$\\nabla f(x) = [x_1 + 1, 2x_2 + \\frac{1}{2}]^\\text{T}, \\ \\mathbf{F}(x) = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix} \\implies x^{*} = [-1,\\frac{-1}{4}]^{\\text{T}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining f, gradient, and Hessian\n",
    "f = lambda x: x[0] + 0.5 * x[1] + 0.5 * (x[0]**2) + (x[1]**2) + 3\n",
    "grad_f = lambda x: np.array([x[0] + 1, 2 * x[1] + 0.5])\n",
    "hess_f = np.array([[1,0],[0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating gradient descent function for functions in quadratic form \n",
    "def grad_descent(g, H, x0, N, tol = 1e-7):\n",
    "    '''\n",
    "    Gradient descent algorithm to find the minimizer of a function f assuming \n",
    "    the function is in quadratic form up to a constant (f(x) = 1/2x^TQx - b^Tx + C)\n",
    "    Inputs:\n",
    "    g - gradient of the function \n",
    "    H - Hessian matrix of the function\n",
    "    x0 - initial guess\n",
    "    N - number of iterations to perform\n",
    "    tol - stopping criteria threshold, algorithm conitnutes until |x_{k+1}-x_k| < tol\n",
    "    '''\n",
    "    x = x0\n",
    "    # count iterations while performing algorithm\n",
    "    iter = 0\n",
    "    while iter < N:\n",
    "        iter += 1\n",
    "        gx = g(x) # calculate gradient at point\n",
    "        alpha = (gx.T @ gx)/(gx.T @ H @ gx) # calculate alpha \n",
    "        new_x = x - (alpha * gx) # calculate next iteration\n",
    "        if iter % 5 == 0:\n",
    "            print(f'Iteration {iter}: x = {new_x}')\n",
    "        if np.linalg.norm(new_x - x) < tol:\n",
    "            break\n",
    "        x = new_x\n",
    "    return x, iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimizer: [-0.92592593 -0.23148148], iterations: 2\n"
     ]
    }
   ],
   "source": [
    "x0 = np.zeros(2)\n",
    "min, iter = grad_descent(grad_f, hess_f, x0, 2)\n",
    "\n",
    "print(f'Minimizer: {min}, iterations: {iter}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient descent algorithm approaches the analytical minimum within 14 iterations. \n",
    "\n",
    "8.2 Suppose the order of convergence is larger than p, i.e. there exists w > p such that $\\\\ 0 < \\lim_{k \\to \\infty}\\frac{||x^{(k+1)}-x^*||}{||x^{(k)}-x^*||^w} < \\infty \\\\$\n",
    "Let z be defined as the limit above, i.e. $z = \\lim_{k \\to \\infty}\\frac{||x^{(k+1)}-x^*||}{||x^{(k)}-x^*||^p||x^{(k)}-x^*||^{w-p}}$. Since $x^{(k)} \\to x^*$, for c > 0 and k large enough $||x^{(k)}-x^*|| < c \\implies ||x^{(k)}-x^*||^{w-p} < c^{w-p}$. This implies \n",
    "\n",
    "$z = \\lim_{k \\to \\infty}\\frac{||x^{(k+1)}-x^*||}{||x^{(k)}-x^*||^p||x^{(k)}-x^*||^{w-p}} > \\lim_{k \\to \\infty}\\frac{||x^{(k+1)}-x^*||}{||x^{(k)}-x^*||^pc^{w-p}} \\implies c^{w-p}z > \\lim_{k \\to \\infty}\\frac{||x^{(k+1)}-x^*||}{||x^{(k)}-x^*||^p}$\n",
    "\n",
    "Thus we can say for sufficiently large k, $c^{w-p}z > \\frac{||x^{(k+1)}-x^*||}{||x^{(k)}-x^*||^p} \\implies ||x^{(k+1)}-x^*|| < c^{w-p}z||x^{(k)}-x^*||^p$. Therefore, the contrapositive is true.\n",
    "\n",
    "8.6 We want to the values of p such that $\\lim_{k \\to \\infty}\\frac{|\\mu_{k+1}|}{|\\mu_k|^p}$ is finite. \n",
    "\n",
    "$\\lim_{k \\to \\infty}\\frac{|\\mu_{k+1}|}{|\\mu_k|^p} = \\lim_{k \\to \\infty}\\frac{\\left(\\frac{\\sqrt{5}-1}{2}\\right)\\mu_k}{\\mu_k^p} = \\left(\\frac{\\sqrt{5}-1}{2}\\right)\\lim_{k \\to \\infty}\\frac{1}{\\mu_k^{p-1}}$\n",
    "\n",
    "Therefore, in order for the limit to converge, we must have p > 2.\n",
    "\n",
    "8.8 First we must compute the the eigenvalues of the Hessian of f. \n",
    "\n",
    "$\\nabla f(\\mathbf{x}) = [6x_1+4x_2+5,6x_2+4x_1+6] \\implies \\mathbf{F}(\\mathbf{x}) = \\begin{bmatrix} 6 & 4 \\\\ 4 & 6 \\end{bmatrix}$\n",
    "\n",
    "$\\implies \\chi_F(\\lambda) = (6-\\lambda)^2-16 = \\lambda^2-12\\lambda -20 \\implies \\lambda_{\\text{max}}(\\mathbf{F}) = 10$\n",
    "\n",
    "Therefore the values of $\\alpha$ which cause the fixed-step algorithm to converge are $0 < \\alpha < \\frac{1}{5}$.\n",
    "\n",
    "8.11 a) \n",
    "$\\begin{align}\n",
    "    f(x^{(k+1)}) &= f(x^{(k)}-\\alpha_k f'(x^{(k)})) \\\\\n",
    "    &= f(x^{(k)} - \\alpha x^{(k)} + \\alpha_k c) \\\\\n",
    "    &= \\frac{1}{2}[(x^{(k)} - \\alpha x^{(k)} + \\alpha_k c) - c]^2 \\\\\n",
    "    &= \\frac{1}{2}[(1-\\alpha_k)(x^{(k)}-c)]^2 \\\\\n",
    "    &= (1-\\alpha_k)^2f(x^{(k)})\n",
    "\\end{align}$\n",
    "\n",
    "b) Analytically, we know c is the minimizer for f. Thus, expanding the iterative formula, we get \n",
    "\n",
    "$x^{(k+1)} = x^{(k)} - \\alpha_k(x^{(k)} - c) \\implies x^{(k+1)} - c = (1 - \\alpha_k)(x^{(k)}-c)$\n",
    "\n",
    "Let $e^{(k)} = x^{(k)} - c$ be the error of the algorithm at iteration k. Using $e^{(k)}$ we get the equation $e^{(k)} = e^{(0)} \\prod_{i=0}^{k-1} (1-\\alpha_i)$.\n",
    "\n",
    "Suppose the algorithm is globally convergent, i.e. $x^{(k)} \\to x^*$ for any $x^{(0)}$. Global convergence implies $e^{(k)} \\to 0$ as $k \\to \\infty$ which would imply $\\prod_{i=0}^{k-1} (1-\\alpha_i) \\to 0$ as $k \\to \\infty$. Thus $\\prod_{i=0}^{\\infty} (1-\\alpha_i) = 0$ which is equivalent to $\\sum_{i=0}^{\\infty}\\alpha_i = \\infty$.\n",
    "\n",
    "Conversely, suppose $\\sum_{i=0}^{\\infty}\\alpha_i = \\infty$. This is equivalent to $\\prod_{i=0}^{\\infty} (1-\\alpha_i) = 0$. Therefore, $e^{(k)} \\to 0$ as $k \\to \\infty$ which implies the algorithm is globally convergent.\n",
    "\n",
    "8.16 a)\n",
    "$\\begin{align}\n",
    "    f(x) &= (Ax-b)^T(Ax-b) \\\\\n",
    "    &= x^TA^TAx - x^TA^Tb - b^TAx + b^Tb \\\\\n",
    "    &= x^TA^TAx - 2(b^TA)x + b^Tb\n",
    "\\end{align}$\n",
    "\n",
    "Since $A^TA$ is symmetric, the objective function takes the form of a quadratic. \n",
    "\n",
    "$\\nabla f(x) =  2A^TAx - 2A^Tb, \\mathbf{F}(x) = 2A^TA$\n",
    "\n",
    "b) $x^{(k+1)} = x^{(k)} - \\alpha\\nabla f(x^{(k)}) = x^{(k)} - 2\\alpha(A^TAx^{(k)} - A^Tb)$\n",
    "\n",
    "c) $A = \\begin{bmatrix} 1 & 0 \\\\ 0 & 2 \\end{bmatrix} \\implies \\mathbf{F} = \\begin{bmatrix} 2 & 0 \\\\ 0 & 8 \\end{bmatrix} \\implies \\lambda_{\\text{max}}(\\mathbf{F}) = 8 \\implies 0 < \\alpha < \\frac{1}{4}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
